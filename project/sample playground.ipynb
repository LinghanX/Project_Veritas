{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init & apply stemmer\n",
      "building vocabulary\n",
      "building tf-idf matrix\n",
      "Saving the TFxIDF matrix and the corresponding values\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import nltk.data\n",
    "import nltk.tokenize\n",
    "import nltk.stem\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def extract_words(text, stemmer = None, remove_stopwords = False):\n",
    "    \"\"\"\n",
    "    Strategy used:\n",
    "    1. Tokenize\n",
    "    2. Stemming\n",
    "    3. Stop word removal\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if stemmer is None:\n",
    "        words = [token.lower() for token in tokens]\n",
    "    else:\n",
    "        words = [stemmer.stem(word.lower()) for word in tokens]\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "    return words\n",
    "\n",
    "def build_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    Creating a list of the total unique worlds found in the corpus.\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    for doc in documents:\n",
    "        vocabulary.update([word for word in doc])\n",
    "    vocabulary = list(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def get_idf_matrix(vocabulary, documents):\n",
    "    \"\"\"\n",
    "        The inverse document frequency acts as a weight for \n",
    "        processing the TF matrix. \n",
    "        \n",
    "        IDF(w) = log((1 + n_d)/ (1 + df(d, t))) + 1\n",
    "        \n",
    "    \"\"\"\n",
    "    n_of_doc = len(documents)\n",
    "    frequency_dic = {} # a dictionary to keep track of df(d, t)\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        for d in documents:\n",
    "            if word in documents:\n",
    "                if word in frequency_dic:\n",
    "                    frequency_dic[word] += 1\n",
    "                else:\n",
    "                    frequency_dic[word] = 1\n",
    "                    \n",
    "    idf = []\n",
    "    \n",
    "    for word in vocabulary:\n",
    "        if word in frequency_dic: \n",
    "            freq = frequency_dic[word]\n",
    "        else:\n",
    "            freq = 0\n",
    "            \n",
    "        idf.append(math.log((float)(1 + n_of_doc) / ((float)(1+ freq)) + 1, 2))\n",
    "        \n",
    "    return scipy.sparse.diags(np.squeeze(np.asarray(idf)))\n",
    "\n",
    "def get_tf_vectors(vocabulary, documents):    \n",
    "    \n",
    "    # starting pointer for row\n",
    "    row_ptr = 0\n",
    "    # data[n] is stored in the matrix[row[n]col[n]]\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    for d in documents:   \n",
    "        col_index = 0\n",
    "        \n",
    "        for word in vocabulary:    \n",
    "            if word in d:\n",
    "                term_freq = d.count(word)\n",
    "                row.append(row_ptr)\n",
    "                col.append(col_index)\n",
    "                data.append(d.count(word))\n",
    "            col_index += 1\n",
    "            \n",
    "        row_ptr += 1\n",
    "\n",
    "    return scipy.sparse.csr_matrix((data, (row, col)), shape=(row_ptr, len(vocabulary)))    \n",
    "\n",
    "def get_log_tf_vectors(vocabulary, documents):    \n",
    "    \n",
    "    # starting pointer for row\n",
    "    row_ptr = 0\n",
    "    # data[n] is stored in the matrix[row[n]col[n]]\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    for d in documents:   \n",
    "        col_index = 0\n",
    "        \n",
    "        for word in vocabulary:    \n",
    "            if word in d:\n",
    "                term_freq = d.count(word)\n",
    "                row.append(row_ptr)\n",
    "                col.append(col_index)\n",
    "                data.append( 1 + math.log(d.count(word), 2))\n",
    "            col_index += 1\n",
    "            \n",
    "        row_ptr += 1\n",
    "\n",
    "    return scipy.sparse.csr_matrix((data, (row, col)), shape=(row_ptr, len(vocabulary)))    \n",
    "\n",
    "\"\"\"\n",
    "    l2_norm implementation taken from\n",
    "    London Machine Learning Study Group: http://www.meetup.com\n",
    "    /London-Machine-Learning-Study-Group/members/\n",
    "\"\"\"\n",
    "def l2_normalized_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Normalises a sparse matrix by scaling its rows individually to L2 unit norm\n",
    "\n",
    "    The new row values are computed as\n",
    "    \n",
    "        ||x|| = sqrt(sum(x^2))\n",
    "        \n",
    "    For efficiency, the resulting new matrix is formed by computing\n",
    "    \n",
    "    normalized_matrix = \n",
    "        transpose(transpose transpose(matrix) * l2_norm)\n",
    "        \n",
    "    where matrix is the original sparse matrix and l2_norm is diagonal \n",
    "    matrix of the reciprocals of sqrt(sum(x^2))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix     : a sparse matrix to be normalized\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An L2 normalised sparse matrix based on the input matrix\n",
    "    \n",
    "    \"\"\"     \n",
    "    # Compute the L2 norms\n",
    "    l2_norm = np.sqrt(matrix.power(2).sum(axis=1))\n",
    "    \n",
    "    # Get the reciprocals\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        l2_norm = np.reciprocal(l2_norm)\n",
    "        # Treat infinity and NaN as 0\n",
    "        l2_norm[~np.isfinite(l2_norm)] = 0  # -inf inf NaN   \n",
    "    \n",
    "    # Form a diagonal matrix of the reciprocals\n",
    "    l2_norm = scipy.sparse.diags(np.squeeze(np.asarray(l2_norm)))           \n",
    "        \n",
    "    # Compute the normalised matrix\n",
    "    normalized_matrix = (matrix.T * l2_norm).T\n",
    "    \n",
    "    return normalized_matrix\n",
    "       \n",
    "def mtx_save(file_name, matrix):\n",
    "    scipy.io.mmwrite(file_name, matrix)\n",
    "\n",
    "def encode_labels(labelsDF):\n",
    "    labelsDF = pd.Categorical(labelsDF)\n",
    "    catLabelsDF = labelsDF.codes\n",
    "\n",
    "    return catLabelsDF\n",
    "\n",
    "def labels_save(file_name, labels):\n",
    "    labels.tofile(file_name, sep='\\n')\n",
    "\n",
    "# Read a data set\n",
    "dataDF = pd.read_csv(\"../data/fake_or_real_news.csv\", \n",
    "                     sep=',',\n",
    "                     nrows=100,\n",
    "                     lineterminator='\\n', names = [\"title\", \"text\", \"label\"])\n",
    "\n",
    "print(\"Init & apply stemmer\")\n",
    "snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "dataDF[\"Words\"] = dataDF.apply(lambda row: extract_words(row['text'], snowball), axis=1)\n",
    "\n",
    "print(\"building vocabulary\")\n",
    "vocabulary = build_vocabulary(dataDF[\"Words\"])\n",
    "\n",
    "tf_matrix = get_tf_vectors(vocabulary, dataDF[\"Words\"])\n",
    "idf_matrix = get_idf_matrix(vocabulary, dataDF[\"Words\"])\n",
    "print(\"building tf-idf matrix\")\n",
    "\n",
    "tf_idf_matrix = tf_matrix * idf_matrix\n",
    "tf_idf_matrix = l2_normalized_matrix(tf_idf_matrix)\n",
    "\n",
    "labels = encode_labels(dataDF[\"label\"])\n",
    "\n",
    "print(\"Saving the TFxIDF matrix and the corresponding values\")\n",
    "mtx_save(\"training.mtx\", tf_idf_matrix)\n",
    "labels_save(\"labels.csv\", labels)\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.001578 sec\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The seperation of feature extraction file from \n",
    "    modelling file is for better modularity of the \n",
    "    project. So that we could plug in different \n",
    "    model for our training data.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import timeit\n",
    "from scipy import io\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "labels = np.fromfile(\"./labels.csv\", sep='\\n')\n",
    "tf_idf_matrix = io.mmread(\"./training.mtx\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=1234)\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "    This file fits the TF-IDF matrix data that we\n",
    "    processsed to the default Multinomial class\n",
    "    \n",
    "    Test results:\n",
    "    \n",
    "    Env: tf_idf_matrix trained using the first 1000 row\n",
    "         test_size: 0.10\n",
    "         random_state: 1234\n",
    "    with L2 normalization:\n",
    "        Using naive term frequency count: 0.69 accuracy\n",
    "        Using log term frequency count: 0.46 accuracy ... hmm.\n",
    "        \n",
    "    without L2:\n",
    "        Using naive term: 0.80\n",
    "        Using log term: 0.45\n",
    "\"\"\"\n",
    "\n",
    "print(\"Elapsed time: %f sec\" % (timeit.default_timer() - start_time))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
