{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming...\n",
      "Building vocabulary...\n",
      "Forming the TF matrix...\n",
      "Forming the IDF matrix...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "(C) 2017 Nikolay Manchev\n",
    "[London Machine Learning Study Group](http://www.meetup.com/London-Machine-Learning-Study-Group/members/)\n",
    "\n",
    "This work is licensed under the Creative Commons Attribution 4.0 International\n",
    "License. To view a copy of this license, visit\n",
    "http://creativecommons.org/licenses/by/4.0/.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "\n",
    "import nltk.data\n",
    "import nltk.tokenize\n",
    "import nltk.stem\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_words(text, stemmer = None, remove_stopwords = False):\n",
    "    \"\"\"\n",
    "    Extracts all words from a document. The document is first tokenized,\n",
    "    morphological affixes from words are removed, and stop words\n",
    "    are excluded from the resulting list of words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text             : input document (String)\n",
    "    stemmer          : NLTK stemmer for the stemming process. Must be an NLTK \n",
    "                       stem package class. E.g:\n",
    "                    \n",
    "                       nltk.stem.porter.PorterStemmer()\n",
    "                       nltk.stem.lancaster.LancasterStemmer()\n",
    "                       nltk.stem.snowball.EnglishStemmer()\n",
    "                      \n",
    "                       If set to None, no stemming is performed on the input text\n",
    "    remove_stopwords : If set to True, removes any stop words from the output,\n",
    "                       using the nltk.corpus.stopwords corpus (English)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of words extracted from the input text.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get the stopwords corpus\n",
    "    #if \"stopwords\" not in os.listdir(nltk.data.find(\"corpora\")):\n",
    "    #    nltk.download(\"stopwords\")\n",
    "\n",
    "    # Tokenize the document\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if stemmer is None:\n",
    "        # No stemmer? Just convert to lower case.\n",
    "        words = [token.lower() for token in tokens]\n",
    "    else:\n",
    "        # Apply stemming    \n",
    "        words = [stemmer.stem(word.lower()) for word in tokens]\n",
    "        \n",
    "    \n",
    "    # Remove stop words\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    return words\n",
    "    \n",
    "def build_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary based on all documents in the corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    documents : document corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list, containing all unique words from the corpus\n",
    "    \n",
    "    \"\"\"    \n",
    "    vocabulary = set()\n",
    "    \n",
    "    # Iterate over each document in the corpus\n",
    "    for doc in documents:\n",
    "        # Iterate over all words in the current document and\n",
    "        # add each word to the vocabulary set\n",
    "        vocabulary.update([word for word in doc])\n",
    "        \n",
    "    # Convert the vocabulary to list\n",
    "    vocabulary = list(vocabulary)\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def get_idfs_dict(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    Gets a dictionary containing the vocabulary and their respective IDFs.\n",
    "    This method is used for debug purposes only.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary : vocabulary of the corpus\n",
    "    documents  : all documents in the corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary in the form of {(word1, word1_IDF), (word2, word2_IDF), ... }\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of documents where each word from the vocabulary appears\n",
    "    counts = Counter()\n",
    "\n",
    "    # Iterate over the vocabulary and count the occurrence of each word\n",
    "    for word in vocabulary:\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                counts[word] += 1\n",
    "    \n",
    "    # Get the number of documents in the corpus\n",
    "    number_of_docs = len(documents)\n",
    "    \n",
    "    # Create an empty dictionary\n",
    "    idfs = dict()\n",
    "\n",
    "    # Iterate over the counts\n",
    "    for term in list(counts.items()):\n",
    "        \n",
    "        # Normalise the count by the number of documents, and take the log\n",
    "        # Add the (word, IDF) pair to the dictionary\n",
    "        idfs[term[0]] = math.log(number_of_docs / term[1], 2)\n",
    "    \n",
    "    return idfs\n",
    "\n",
    "\n",
    "def get_idfs(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    Gets a sparse diagonal matrix containing the IDFs for all words in the\n",
    "    vocabulary. The IDF are computed as the logarithmically scaled inverse \n",
    "    fraction of the documents that contain the word, obtained by dividing \n",
    "    the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that quotient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary : vocabulary of the corpus\n",
    "    documents  : all documents in the corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A diagonal matrix of size len(vocabulary) x len(vocabulary), where the\n",
    "    word's IDFs are located on the main diagonal, and all other elements in\n",
    "    the matrix are 0. Eg:\n",
    "    \n",
    "    index in the vocabulary    0      1      2      3     ...     N\n",
    "                               1 idf(word1)  0      0     ...     0  \n",
    "                               2      0  idf(word2) 0     ...     0  \n",
    "                               3      0      0 idf(word3) ...     0\n",
    "                             ...     ...    ...    ...    ...    ...\n",
    "                               N      0       0      0     ... idf(wordN)\n",
    "                  \n",
    "    where N = len(vocabulary)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of documents where each word from the vocabulary appears\n",
    "    counts = dict()\n",
    "\n",
    "    for word in vocabulary:\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                if word in counts:\n",
    "                  counts[word] += 1\n",
    "                else:\n",
    "                  counts[word] = 1\n",
    "    \n",
    "    # Compute inverse document frequency\n",
    "    number_of_docs = len(documents)\n",
    "    \n",
    "    # Create a list to hold all the IDFs\n",
    "    idfs = []\n",
    "    \n",
    "    # Iterate over the counts\n",
    "    for word in vocabulary:\n",
    "        \n",
    "        # Normalise the count by the number of documents, and take the log\n",
    "        # Add the value to the list of IDFs\n",
    "        idfs.append(math.log(number_of_docs / counts[word], 2))\n",
    "\n",
    "    # Create a sparse diagonal matrix with the values from IDFs list located\n",
    "    # on the main diagonal\n",
    "    idf_matrix = scipy.sparse.diags(np.squeeze(np.asarray(idfs)))\n",
    "\n",
    "    return idf_matrix    \n",
    "\n",
    "def get_tf_vectors(vocabulary, documents):    \n",
    "    \"\"\"\n",
    "    Computes the term frequency vectors for all documents. This method uses\n",
    "    raw count of a term in a document, i.e. the number of times that term \n",
    "    t occurs in document d.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary : vocabulary of the corpus\n",
    "    documents  : all documents in the corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A sparse matrix of size len(documents) x len(vocabulary), containing the\n",
    "    raw counts for each term. Entries in the matrix can be viewed using\n",
    "    the print_sparse_row(matrix, row_index) method. Ex:\n",
    "\n",
    "    tf_matrix.shape\n",
    "    (6918, 1869)\n",
    "\n",
    "    print_sparse_row(tf_matrix,0)\n",
    "    col[106] 1\n",
    "    col[289] 1\n",
    "    col[482] 1\n",
    "    col[815] 1\n",
    "    col[1074] 1\n",
    "    col[1145] 1\n",
    "    col[1232] 1\n",
    "    col[1565] 1    \n",
    "    \"\"\"\n",
    "\n",
    "    # Document / sparse matrix row index\n",
    "    row_index = 0\n",
    "    \n",
    "    # Values and indices for the sparse matrix\n",
    "    rows = []\n",
    "    cols = []\n",
    "    values = []\n",
    "\n",
    "    # Iterate over all documents in the corpus\n",
    "    for doc in documents:        \n",
    "        col_index = 0\n",
    "        \n",
    "        # Iterate over all words in the vocabulary\n",
    "        for word in vocabulary:\n",
    "            \n",
    "            # Current word in current document?\n",
    "            if word in doc:\n",
    "                # Increase the term frequency for this word\n",
    "                rows.append(row_index)\n",
    "                cols.append(col_index)\n",
    "                values.append(doc.count(word))\n",
    "                col_index += 1\n",
    "            else:\n",
    "                # Move to the next word in the vocabulary\n",
    "                col_index += 1\n",
    "                \n",
    "        # Move to the next document\n",
    "        row_index += 1\n",
    "    \n",
    "    # Compose a sparse matrix of size len(documents) x len(vocabulary) with\n",
    "    # all term frequencies\n",
    "    tf_matrix = scipy.sparse.csr_matrix((values, (rows, cols)), shape=(row_index, len(vocabulary)))\n",
    "    \n",
    "    return tf_matrix\n",
    "\n",
    "def print_sparse_row(matrix, row_index):\n",
    "    \"\"\"\n",
    "    Prints the indices and their respective values for a sparse matrix row.\n",
    "    This method is used for debugging purposes.    \n",
    "    \n",
    "    Ex:\n",
    "\n",
    "    print_sparse_row(tf_matrix,0)\n",
    "    col[106] 1\n",
    "    col[289] 1\n",
    "    col[482] 1\n",
    "    col[815] 1\n",
    "    col[1074] 1\n",
    "    col[1145] 1\n",
    "    col[1232] 1\n",
    "    col[1565] 1    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix     : a sparse matrix\n",
    "    row_index  : index of a row from the sparse matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert the row of interest to a Numpy array\n",
    "    row = np.asarray(matrix[row_index].todense()).flatten()\n",
    "    \n",
    "    # Iterate over all columns of the row\n",
    "    col = 0\n",
    "    for el in row:\n",
    "        if el != 0:\n",
    "            # Print the column index and the respective value\n",
    "            print(\"col[%i] %s\"%(col, el))\n",
    "        col += 1\n",
    "\n",
    "\n",
    "def print_tfidf(matrix, row_index, idfs):\n",
    "    \"\"\"\n",
    "    For a given row from a TF matrix, this method prints a table containing\n",
    "    all words, their term frequency, IDF, and TFxIDF values. Ex:\n",
    "    \n",
    "    >>> idfs = get_idfs_dict(vocabulary, dataDF[\"Words\"])\n",
    "    >>> print_tfidf(tf_matrix, 0, idfs)\n",
    "\n",
    "    Column   Word      TF      IDF                 TFxIDF              \n",
    "    ------   ----      --      ---                 ------              \n",
    "    106      is        1       2.2355206178166482  2.23552061782       \n",
    "    289      just      1       4.616587945974135   4.61658794597       \n",
    "    482      the       1       1.448369266482225   1.44836926648       \n",
    "    815      vinc      1       1.8033980511864398  1.80339805119       \n",
    "    1074     da        1       1.8033980511864398  1.80339805119       \n",
    "    1145     book      1       5.434211203485566   5.43421120349       \n",
    "    1232     code      1       1.801942987986053   1.80194298799       \n",
    "    1565     awesom    1       2.6320179865437403  2.63201798654  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix     : matrix containg document term frequencies (see the \n",
    "                 get_tf_vectors method)\n",
    "    row_index  : index of a row from the TF matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Get the row of interest as a Numpy array\n",
    "    row = np.asarray(matrix[row_index].todense()).flatten()\n",
    "    col = 0\n",
    "    \n",
    "    # Set the output header\n",
    "    output = [[\"Column\", \"Word\", \"TF\", \"IDF\", \"TFxIDF\"],\n",
    "              [\"------\", \"----\", \"--\", \"---\", \"------\"]]\n",
    "    \n",
    "    # Go over each element of the row (i.e. word from the document)\n",
    "    for el in row:\n",
    "        if el != 0:\n",
    "          # Append the column index, the word, and the TF, IDF, and TFxIDF\n",
    "          # values to the output\n",
    "          output.append([str(col), vocabulary[col], str(el), str(idfs[vocabulary[col]]), \n",
    "                         str(idfs[vocabulary[col]]*el)])\n",
    "        col += 1\n",
    "                \n",
    "    # Print the output as a table\n",
    "    col_width = max(len(word) for row in output for word in row) + 2  # padding\n",
    "    for row in output:\n",
    "      print(\"\".join(word.ljust(col_width) for word in row))\n",
    "\n",
    "def l2_normalized_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Normalises a sparse matrix by scaling its rows individually to L2 unit norm\n",
    "\n",
    "    The new row values are computed as\n",
    "    \n",
    "        ||x|| = sqrt(sum(x^2))\n",
    "        \n",
    "    For efficiency, the resulting new matrix is formed by computing\n",
    "    \n",
    "    normalized_matrix = \n",
    "        transpose(transpose transpose(matrix) * l2_norm)\n",
    "        \n",
    "    where matrix is the original sparse matrix and l2_norm is diagonal \n",
    "    matrix of the reciprocals of sqrt(sum(x^2))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix     : a sparse matrix to be normalized\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An L2 normalised sparse matrix based on the input matrix\n",
    "    \n",
    "    \"\"\"     \n",
    "    # Compute the L2 norms\n",
    "    l2_norm = np.sqrt(matrix.power(2).sum(axis=1))\n",
    "    \n",
    "    # Get the reciprocals\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        l2_norm = np.reciprocal(l2_norm)\n",
    "        # Treat infinity and NaN as 0\n",
    "        l2_norm[~np.isfinite(l2_norm)] = 0  # -inf inf NaN   \n",
    "    \n",
    "    # Form a diagonal matrix of the reciprocals\n",
    "    l2_norm = scipy.sparse.diags(np.squeeze(np.asarray(l2_norm)))           \n",
    "        \n",
    "    # Compute the normalised matrix\n",
    "    normalized_matrix = (matrix.T * l2_norm).T\n",
    "    \n",
    "    return normalized_matrix\n",
    "       \n",
    "def mtx_save(file_name, matrix):\n",
    "    \"\"\"\n",
    "    Writes a sparse matrix a to Matrix Market file-like target.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : target file name\n",
    "    matrix    : a sparse matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    scipy.io.mmwrite(file_name, matrix)\n",
    "\n",
    "def encode_labels(labelsDF):\n",
    "    \"\"\"\n",
    "    Encodes a string set of target classes to a Numpy array of label indices\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labelsDF : a Pandas DataFrame or Numpy array containing the labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An encoded Numpy array\n",
    "    \n",
    "    Ex:\n",
    "    \n",
    "    >>> A = np.array([\"a\", \"a\", \"b\", \"a\"])\n",
    "    >>> encode_labels(A)\n",
    "        array([0, 0, 1, 0], dtype=int8)\n",
    "        \n",
    "    \"\"\"    \n",
    "    # Factorize the labels\n",
    "    labelsDF = pd.Categorical(labelsDF)\n",
    "    catLabelsDF = labelsDF.codes\n",
    "\n",
    "    return catLabelsDF\n",
    "\n",
    "def labels_save(file_name, labels):\n",
    "    \"\"\"\n",
    "    Saves the target class labels to an external file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : target file name\n",
    "    labels    : a Numpy array containing the labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        \n",
    "    \"\"\"       \n",
    "    labels.tofile(file_name, sep='\\n')\n",
    "\n",
    "def hash_vectors(tf_idf_matrix, vocabulary, N=8000):\n",
    "    \"\"\"\n",
    "    Applies feature hashing / hashing trick to a sparse matrix. This method\n",
    "    turns features into indices in a vector or matrix. It works by applying a \n",
    "    hash function to the features and using their hash values as indices \n",
    "    directly, rather than looking the indices up in an associative array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tf_idf_matrix : a sparse matrix of TFxIDF values\n",
    "    vocabulary    : vocabulary of the corpus\n",
    "    N             : size of the hased vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A sparse matrix of size tf_idf_matrix.shape[0] x N, containing the hashed\n",
    "    features\n",
    "    \n",
    "    >>> tf_idf_matrix.shape\n",
    "        (6918, 1869)\n",
    "\n",
    "    >>> hash_vectors(tf_idf_matrix, vocabulary, 100).shape\n",
    "        (6918, 100)\n",
    "            \n",
    "    \"\"\"       \n",
    "    \n",
    "    # Make sure the input is a csr_matrix (wee need to access the sparse\n",
    "    # matrix elements directly )\n",
    "    if not isinstance(tf_idf_matrix, scipy.sparse.csr.csr_matrix):\n",
    "      print(\"WARN: Input %s is not a Compressed Sparse Row matrix. Converting...\")      \n",
    "      tf_idf_matrix = tf_idf_matrix.tocsr()\n",
    "\n",
    "\n",
    "    row_count = tf_idf_matrix.shape[0]\n",
    "  \n",
    "    hashed_rows = []\n",
    "    hashed_cols = []\n",
    "    hashed_data = []\n",
    "  \n",
    "    # Iterate over the matrix rows\n",
    "    for row_index in range(row_count):\n",
    "      \n",
    "      # Get the current row indices\n",
    "      row = tf_idf_matrix.getrow(row_index)\n",
    "      col_indices = row.indices\n",
    "      \n",
    "      # Iterate over the columns\n",
    "      for col_index in range(len(col_indices)):\n",
    "          # Get the word and its corresponding TFxIDF value\n",
    "          tf_idf_value = tf_idf_matrix[row_index, col_indices[col_index]]\n",
    "          word = vocabulary[col_indices[col_index]]\n",
    "          \n",
    "          # Apply a hash function h to the features (e.g., words), then use \n",
    "          # the hash values directly as feature indices and update the\n",
    "          # resulting vector at those indices\n",
    "\n",
    "          h = hash(word)\n",
    "          hashed_rows.append(row_index)\n",
    "          hashed_cols.append(h % N)\n",
    "          hashed_data.append(tf_idf_value)\n",
    "      \n",
    "    # Create a new sparse matrix with the hashed features\n",
    "    hashed_features_matrix = scipy.sparse.csr_matrix((hashed_data, \n",
    "                                                     (hashed_rows, hashed_cols)), \n",
    "                                                   shape=(row_count, N))\n",
    "                                       \n",
    "    return hashed_features_matrix                                \n",
    "\n",
    "\n",
    "# Read a data set\n",
    "dataDF = pd.read_csv(\"../data/fake_or_real_news.csv\", \n",
    "                     sep=',', lineterminator='\\n', names = [\"\", \"title\", \"text\", \"label\"])\n",
    "\n",
    "# Initialise a stemmer\n",
    "\n",
    "#porter = nltk.stem.porter.PorterStemmer()\n",
    "#lancaster = nltk.stem.lancaster.LancasterStemmer()\n",
    "snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "print(\"Stemming...\")\n",
    "dataDF[\"Words\"] = dataDF.apply(lambda row: extract_words(row['text'], snowball), axis=1)\n",
    "\n",
    "# Remove empty rows. Messages like \":)\" which will get removed by the stemmer\n",
    "#dataDF = dataDF[dataDF.astype(str)[\"words\"] != '[]']\n",
    "#dataDF = dataDF.reset_index(drop=True)\n",
    "\n",
    "# Build a vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "vocabulary = build_vocabulary(dataDF[\"Words\"])\n",
    "\n",
    "# Get the TF vectors\n",
    "print(\"Forming the TF matrix...\")\n",
    "tf_matrix = get_tf_vectors(vocabulary, dataDF[\"Words\"])\n",
    "\n",
    "# Get the IDF matrix\n",
    "print(\"Forming the IDF matrix...\")\n",
    "idf_matrix = get_idfs(vocabulary, dataDF[\"Words\"])\n",
    "\n",
    "# Compute the TFxIDF values\n",
    "print(\"Computing the TFxIDF matrix...\")\n",
    "tf_idf_matrix = (tf_matrix * idf_matrix)\n",
    "tf_idf_matrix = l2_normalized_matrix(tf_idf_matrix)\n",
    "\n",
    "#tf_idf_matrix = hash_vectors(tf_idf_matrix, vocabulary, 125)\n",
    "\n",
    "# Encode the labels\n",
    "print(\"Encoding labels...\")\n",
    "labels = encode_labels(dataDF[\"label\"])\n",
    "\n",
    "# Save the TFxIDF matrix and the corresponding values\n",
    "print(\"Saving features and labels...\")\n",
    "mtx_save(\"training.mtx\", tf_idf_matrix)\n",
    "labels_save(\"labels.csv\", labels)\n",
    "\n",
    "print(\"all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import timeit\n",
    "from scipy import io\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "labels = np.fromfile(\"./labels.csv\", sep='\\n')\n",
    "tf_idf_matrix = io.mmread(\"./training.mtx\").todense()\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=1234)\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Elapsed time: %f sec\" % (timeit.default_timer() - start_time))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
