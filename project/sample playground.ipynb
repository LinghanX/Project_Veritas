{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming...\n",
      "Building vocabulary...\n",
      "Forming the TF matrix...\n",
      "Forming the IDF matrix...\n",
      "Computing the TFxIDF matrix...\n",
      "Encoding labels...\n",
      "Saving features and labels...\n",
      "all done!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import nltk.data\n",
    "import nltk.tokenize\n",
    "import nltk.stem\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def extract_words(text, stemmer = None, remove_stopwords = False):\n",
    "    \"\"\"\n",
    "    Strategy used:\n",
    "    1. Tokenize\n",
    "    2. Stemming\n",
    "    3. Stop word removal\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if stemmer is None:\n",
    "        words = [token.lower() for token in tokens]\n",
    "    else:\n",
    "        words = [stemmer.stem(word.lower()) for word in tokens]\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "    return words\n",
    "\n",
    "def build_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    Creating a list of the total unique worlds found in the corpus.\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    for doc in documents:\n",
    "        vocabulary.update([word for word in doc])\n",
    "    vocabulary = list(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def get_idfs(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    Gets a sparse diagonal matrix containing the IDFs for all words in the\n",
    "    vocabulary. The IDF are computed as the logarithmically scaled inverse \n",
    "    fraction of the documents that contain the word, obtained by dividing \n",
    "    the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that quotient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocabulary : vocabulary of the corpus\n",
    "    documents  : all documents in the corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A diagonal matrix of size len(vocabulary) x len(vocabulary), where the\n",
    "    word's IDFs are located on the main diagonal, and all other elements in\n",
    "    the matrix are 0. Eg:\n",
    "    \n",
    "    index in the vocabulary    0      1      2      3     ...     N\n",
    "                               1 idf(word1)  0      0     ...     0  \n",
    "                               2      0  idf(word2) 0     ...     0  \n",
    "                               3      0      0 idf(word3) ...     0\n",
    "                             ...     ...    ...    ...    ...    ...\n",
    "                               N      0       0      0     ... idf(wordN)\n",
    "                  \n",
    "    where N = len(vocabulary)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of documents where each word from the vocabulary appears\n",
    "    counts = dict()\n",
    "\n",
    "    for word in vocabulary:\n",
    "        for doc in documents:\n",
    "            if word in doc:\n",
    "                if word in counts:\n",
    "                  counts[word] += 1\n",
    "                else:\n",
    "                  counts[word] = 1\n",
    "    \n",
    "    # Compute inverse document frequency\n",
    "    number_of_docs = len(documents)\n",
    "    \n",
    "    # Create a list to hold all the IDFs\n",
    "    idfs = []\n",
    "    \n",
    "    # Iterate over the counts\n",
    "    for word in vocabulary:\n",
    "        \n",
    "        # Normalise the count by the number of documents, and take the log\n",
    "        # Add the value to the list of IDFs\n",
    "        idfs.append(math.log(number_of_docs / counts[word], 2))\n",
    "\n",
    "    # Create a sparse diagonal matrix with the values from IDFs list located\n",
    "    # on the main diagonal\n",
    "    idf_matrix = scipy.sparse.diags(np.squeeze(np.asarray(idfs)))\n",
    "\n",
    "    return idf_matrix    \n",
    "\n",
    "def get_tf_vectors(vocabulary, documents):    \n",
    "    \n",
    "    # starting pointer for row\n",
    "    row_ptr = 0\n",
    "    # data[n] is stored in the matrix[row[n]col[n]]\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    for d in documents:   \n",
    "        col_index = 0\n",
    "        \n",
    "        for word in vocabulary:    \n",
    "            if word in d:\n",
    "                term_freq = d.count(word)\n",
    "                row.append(row_ptr)\n",
    "                col.append(col_index)\n",
    "                data.append(d.count(word))\n",
    "            col_index += 1\n",
    "            \n",
    "        row_ptr += 1\n",
    "\n",
    "    return scipy.sparse.csr_matrix((data, (row, col)), shape=(row_ptr, len(vocabulary)))    \n",
    "\n",
    "def get_log_tf_vectors(vocabulary, documents):    \n",
    "    \n",
    "    # starting pointer for row\n",
    "    row_ptr = 0\n",
    "    # data[n] is stored in the matrix[row[n]col[n]]\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    for d in documents:   \n",
    "        col_index = 0\n",
    "        \n",
    "        for word in vocabulary:    \n",
    "            if word in d:\n",
    "                term_freq = d.count(word)\n",
    "                row.append(row_ptr)\n",
    "                col.append(col_index)\n",
    "                data.append( 1 + math.log(d.count(word), 2))\n",
    "            col_index += 1\n",
    "            \n",
    "        row_ptr += 1\n",
    "\n",
    "    return scipy.sparse.csr_matrix((data, (row, col)), shape=(row_ptr, len(vocabulary)))    \n",
    "\n",
    "\n",
    "def l2_normalized_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Normalises a sparse matrix by scaling its rows individually to L2 unit norm\n",
    "\n",
    "    The new row values are computed as\n",
    "    \n",
    "        ||x|| = sqrt(sum(x^2))\n",
    "        \n",
    "    For efficiency, the resulting new matrix is formed by computing\n",
    "    \n",
    "    normalized_matrix = \n",
    "        transpose(transpose transpose(matrix) * l2_norm)\n",
    "        \n",
    "    where matrix is the original sparse matrix and l2_norm is diagonal \n",
    "    matrix of the reciprocals of sqrt(sum(x^2))\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix     : a sparse matrix to be normalized\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An L2 normalised sparse matrix based on the input matrix\n",
    "    \n",
    "    \"\"\"     \n",
    "    # Compute the L2 norms\n",
    "    l2_norm = np.sqrt(matrix.power(2).sum(axis=1))\n",
    "    \n",
    "    # Get the reciprocals\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        l2_norm = np.reciprocal(l2_norm)\n",
    "        # Treat infinity and NaN as 0\n",
    "        l2_norm[~np.isfinite(l2_norm)] = 0  # -inf inf NaN   \n",
    "    \n",
    "    # Form a diagonal matrix of the reciprocals\n",
    "    l2_norm = scipy.sparse.diags(np.squeeze(np.asarray(l2_norm)))           \n",
    "        \n",
    "    # Compute the normalised matrix\n",
    "    normalized_matrix = (matrix.T * l2_norm).T\n",
    "    \n",
    "    return normalized_matrix\n",
    "       \n",
    "def mtx_save(file_name, matrix):\n",
    "    \"\"\"\n",
    "        produce a sparse matrix for the processed data\n",
    "    \"\"\"\n",
    "    scipy.io.mmwrite(file_name, matrix)\n",
    "\n",
    "def encode_labels(labelsDF):\n",
    "    # Factorize the labels\n",
    "    labelsDF = pd.Categorical(labelsDF)\n",
    "    catLabelsDF = labelsDF.codes\n",
    "\n",
    "    return catLabelsDF\n",
    "\n",
    "def labels_save(file_name, labels):\n",
    "    labels.tofile(file_name, sep='\\n')\n",
    "\n",
    "# Read a data set\n",
    "dataDF = pd.read_csv(\"../data/fake_or_real_news.csv\", \n",
    "                     sep=',',\n",
    "                     nrows=100,\n",
    "                     lineterminator='\\n', names = [\"title\", \"text\", \"label\"])\n",
    "\n",
    "# Initialise a stemmer\n",
    "\n",
    "#porter = nltk.stem.porter.PorterStemmer()\n",
    "#lancaster = nltk.stem.lancaster.LancasterStemmer()\n",
    "snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "print(\"Stemming...\")\n",
    "dataDF[\"Words\"] = dataDF.apply(lambda row: extract_words(row['text'], snowball), axis=1)\n",
    "\n",
    "# Remove empty rows. Messages like \":)\" which will get removed by the stemmer\n",
    "#dataDF = dataDF[dataDF.astype(str)[\"words\"] != '[]']\n",
    "#dataDF = dataDF.reset_index(drop=True)\n",
    "\n",
    "# Build a vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "vocabulary = build_vocabulary(dataDF[\"Words\"])\n",
    "\n",
    "# Get the TF vectors\n",
    "print(\"Forming the TF matrix...\")\n",
    "tf_matrix = get_tf_vectors(vocabulary, dataDF[\"Words\"])\n",
    "\n",
    "# Get the IDF matrix\n",
    "print(\"Forming the IDF matrix...\")\n",
    "idf_matrix = get_idfs(vocabulary, dataDF[\"Words\"])\n",
    "\n",
    "# Compute the TFxIDF values\n",
    "print(\"Computing the TFxIDF matrix...\")\n",
    "tf_idf_matrix = (tf_matrix * idf_matrix)\n",
    "tf_idf_matrix = l2_normalized_matrix(tf_idf_matrix)\n",
    "\n",
    "# Encode the labels\n",
    "print(\"Encoding labels...\")\n",
    "labels = encode_labels(dataDF[\"label\"])\n",
    "\n",
    "# Save the TFxIDF matrix and the corresponding values\n",
    "print(\"Saving features and labels...\")\n",
    "mtx_save(\"training.mtx\", tf_idf_matrix)\n",
    "labels_save(\"labels.csv\", labels)\n",
    "\n",
    "print(\"all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.001488 sec\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The seperation of feature extraction file from \n",
    "    modelling file is for better modularity of the \n",
    "    project. So that we could plug in different \n",
    "    model for our training data.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import timeit\n",
    "from scipy import io\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "labels = np.fromfile(\"./labels.csv\", sep='\\n')\n",
    "#tf_idf_matrix = io.mmread(\"./training.mtx\").todense()\n",
    "tf_idf_matrix = io.mmread(\"./training.mtx\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_matrix, \n",
    "                                                    labels, \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=1234)\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "    This file fits the TF-IDF matrix data that we\n",
    "    processsed to the default Multinomial class\n",
    "    \n",
    "    Test results:\n",
    "    \n",
    "    Env: tf_idf_matrix trained using the first 1000 row\n",
    "         test_size: 0.10\n",
    "         random_state: 1234\n",
    "    with L2 normalization:\n",
    "        Using naive term frequency count: 0.69 accuracy\n",
    "        Using log term frequency count: 0.46 accuracy ... hmm.\n",
    "        \n",
    "    without L2:\n",
    "        Using naive term: 0.80\n",
    "        Using log term: 0.45\n",
    "\"\"\"\n",
    "\n",
    "print(\"Elapsed time: %f sec\" % (timeit.default_timer() - start_time))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
