{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Rows in Source:Training:Test 15 12 3\n",
      "Stemming, stopwords removal from data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:229: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary\n",
      "Calculating priors\n",
      "Building Label Word count Matrix\n",
      "Writing Trained Model to file\n",
      "Calculating Accuracy Results\n",
      "Accuracy % 66.6666666667\n",
      "Writing resulting labels to file\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import nltk.tokenize\n",
    "import nltk.stem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "SMOOTHNING_FACTOR = 1\n",
    "world_label_matrix = []\n",
    "NUM_OF_LABELS = 2\n",
    "# Traning data consists of 70 % of the original\n",
    "# input file\n",
    "TRAINING_SPLIT_PERCENT = 0.70\n",
    "DATA_FILE = \"../data/fake_or_real_news_nb.csv\"\n",
    "TRAINING_FILE = \"../data/training_naive_bayes.json\"\n",
    "\n",
    "label_index = {\n",
    "    \"FAKE\": 0,\n",
    "    \"REAL\": 1\n",
    "}\n",
    "\n",
    "def extract_words(text, stemmer = None, remove_stopwords = False):\n",
    "    \"\"\"\n",
    "    Strategy used:\n",
    "    1. Tokenize\n",
    "    2. Stemming\n",
    "    3. Stop word removal\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if stemmer is None:\n",
    "        words = [token.lower() for token in tokens]\n",
    "    else:\n",
    "        words = [stemmer.stem(word.lower()) for word in tokens]\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "def build_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    Creating a list of the total unique worlds found in the corpus.\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    for doc in documents:\n",
    "        vocabulary.update([word for word in doc if word != \"text\"])\n",
    "    vocabulary = list(vocabulary)\n",
    "    return vocabulary\n",
    "\n",
    "def get_word_count_by_label(documents):\n",
    "    \"\"\"\n",
    "    Returns the total number of words in each class including\n",
    "    muliple occurences of the same world\n",
    "    \"\"\"\n",
    "    label_word_count = {\n",
    "        \"FAKE\": 0,\n",
    "        \"REAL\": 0\n",
    "    }\n",
    "    for index, row in documents.iterrows():\n",
    "        # to skip the first line which is of not importance to us\n",
    "        if row[\"label\"].strip().upper() == \"LABEL\":\n",
    "            continue\n",
    "        else:\n",
    "            label_word_count[row[\"label\"].strip().upper()] += len(row[\"Transformed text\"])\n",
    "    return label_word_count\n",
    "\n",
    "def calculate_prior_probabilities(documents):\n",
    "    \"\"\"\n",
    "    Returns the Prior probabilies of FAKE/REAL classes from the corpus\n",
    "    \"\"\"\n",
    "    label_priors = {\n",
    "        \"FAKE\": 0,\n",
    "        \"REAL\": 0\n",
    "    }\n",
    "    for index, row in documents.iterrows():\n",
    "        # to skip the first line which is of not importance to us\n",
    "        if row[\"label\"].strip().upper() == \"LABEL\":\n",
    "            continue\n",
    "        else:\n",
    "            label_priors[row[\"label\"].strip().upper()] += 1\n",
    "    return label_priors\n",
    "\n",
    "def init_label_word_count_matrix(num_of_words):\n",
    "    matrix = []\n",
    "    for i in range(num_of_words):\n",
    "        matrix.append([0 for n in range(NUM_OF_LABELS)])\n",
    "    return matrix\n",
    "\n",
    "def label_word_count_matrix(vocabulary, documents):\n",
    "    num_of_words = len(vocabulary)\n",
    "    matrix = init_label_word_count_matrix(num_of_words)\n",
    "    for row_index in range(num_of_words):\n",
    "        for index, row in documents.iterrows():\n",
    "            cleaned_label = row[\"label\"].strip().upper()\n",
    "            if cleaned_label == \"LABEL\":\n",
    "                continue\n",
    "            col_index = label_index[cleaned_label]\n",
    "            matrix[row_index][col_index] += row[\"Transformed text\"].count(vocabulary[row_index])\n",
    "    return matrix\n",
    "\n",
    "def prob_of_wrd_gvn_class(\n",
    "    vocabulary, fake_word_count, real_word_count, label_word_matrix, word):\n",
    "    \"\"\"\n",
    "    Computes the probabilities for the given text for given class\n",
    "    \"\"\"\n",
    "    num_of_words = len(vocabulary)\n",
    "    try:\n",
    "        word_index = vocabulary.index(word)\n",
    "        word_frequency_in_fake_class = label_word_matrix[word_index][label_index[\"FAKE\"]]\n",
    "        word_frequency_in_real_class = label_word_matrix[word_index][label_index[\"REAL\"]]\n",
    "    except ValueError as err:\n",
    "        # word is not present in the vocabulary contructed\n",
    "        word_frequency_in_fake_class = word_frequency_in_real_class = 0\n",
    "    prob_of_wrd_gvn_fake = float(\n",
    "        word_frequency_in_fake_class + SMOOTHNING_FACTOR)/float(\n",
    "        fake_word_count + num_of_words)\n",
    "    prob_of_wrd_gvn_real = float(\n",
    "        word_frequency_in_real_class + SMOOTHNING_FACTOR)/float(\n",
    "        real_word_count + num_of_words)\n",
    "    return prob_of_wrd_gvn_fake, prob_of_wrd_gvn_real\n",
    "\n",
    "def evaluateForAccuracy(testingDF, prior_of_fake, prior_of_real):\n",
    "    \"\"\"\n",
    "    Testing dataFrame will get generated from the original\n",
    "    Dataframe. Which constitutes the last 1901 rows\n",
    "    of the original Dataframe\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    trained_file = open(TRAINING_FILE, \"r\")\n",
    "    trainingData = json.loads(trained_file.read())\n",
    "    fake_word_count = 0\n",
    "    real_word_count = 0\n",
    "    total_words = 0\n",
    "    trainingDataDict = {}\n",
    "\n",
    "    # calculate the essentials from the trained dataset\n",
    "    for word, freq in trainingData:\n",
    "        total_words+=1\n",
    "        fake, real = freq\n",
    "        fake_word_count+=int(fake)\n",
    "        real_word_count+=int(real)\n",
    "        trainingDataDict[word] = (\n",
    "            int(fake), int(real))\n",
    "\n",
    "    # getLabels for each row in the testDataFrame\n",
    "    # calculates the accuracy count for predicted labels\n",
    "    accuracyCount = 0\n",
    "    snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "    for index, row in testingDF.iterrows():\n",
    "        words = extract_words(row[\"text\"], snowball, True)\n",
    "        product_gvn_fake = prior_of_fake\n",
    "        product_gvn_real = prior_of_real\n",
    "        label = None\n",
    "        for word in words:\n",
    "            freq_tuple = trainingDataDict.get(word, None)\n",
    "            fake_freq, real_freq = freq_tuple if freq_tuple else (0, 0)\n",
    "            prob_of_wrd_gvn_fake = float(\n",
    "                fake_freq + SMOOTHNING_FACTOR)/float(\n",
    "                fake_word_count + total_words)\n",
    "            prob_of_wrd_gvn_real = float(\n",
    "                real_freq + SMOOTHNING_FACTOR)/float(\n",
    "                real_word_count + total_words)\n",
    "            product_gvn_fake*=prob_of_wrd_gvn_fake\n",
    "            product_gvn_real*=prob_of_wrd_gvn_real\n",
    "        if product_gvn_real > product_gvn_fake:\n",
    "            label = \"REAL\"\n",
    "        else:\n",
    "            label = \"FAKE\"\n",
    "        if label == row[\"label\"].strip().upper():\n",
    "            accuracyCount+=1\n",
    "        results.append(label)\n",
    "    # Number of rows labeled accuratly/ Total number of rows\n",
    "    print \"Accuracy %\",float(accuracyCount)/float(len(testingDF))*100\n",
    "    return results\n",
    "\n",
    "\n",
    "def multinomial_NBC(\n",
    "    new_sample, vocabulary, fake_wrd_cnt, real_wrd_cnt, label_wrd_matrix, prior_of_fake, prior_of_real):\n",
    "    words = extract_words(new_sample)\n",
    "    product_gvn_fake = prior_of_fake\n",
    "    product_gvn_real = prior_of_real\n",
    "    for word in words:\n",
    "        prob_of_wrd_gvn_fake, prob_of_wrd_gvn_real = prob_of_wrd_gvn_class(\n",
    "                vocabulary,\n",
    "                fake_wrd_cnt,\n",
    "                real_wrd_cnt,\n",
    "                label_wrd_matrix, word)\n",
    "        product_gvn_fake*=prob_of_wrd_gvn_fake\n",
    "        product_gvn_real*=prob_of_wrd_gvn_real\n",
    "\n",
    "    print \"Probability of word|fake\", product_gvn_fake\n",
    "    print \"Probability of word|real\", product_gvn_real\n",
    "    if product_gvn_fake > product_gvn_real:\n",
    "        return \"FAKE\"\n",
    "    else:\n",
    "        return \"REAL\"\n",
    "\n",
    "def get_priors(dataDF):\n",
    "    label_priors = calculate_prior_probabilities(dataDF)\n",
    "    prior_of_real = float(label_priors[\"REAL\"])/float(label_priors[\"REAL\"] + label_priors[\"FAKE\"])\n",
    "    prior_of_fake = float(label_priors[\"FAKE\"])/float(label_priors[\"REAL\"] + label_priors[\"FAKE\"])\n",
    "    return prior_of_real, prior_of_fake\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        dataDF_1 = pd.read_csv(\n",
    "            DATA_FILE,\n",
    "            sep=',', lineterminator='\\n',\n",
    "            names = [\"title\", \"text\", \"label\"], encoding=\"utf-8\")\n",
    "\n",
    "        # dataDF is used for training\n",
    "        dataDF = dataDF_1.copy()\n",
    "        dataDF = dataDF_1.head(\n",
    "            int(TRAINING_SPLIT_PERCENT * len(dataDF_1)))\n",
    "\n",
    "        #testingDF is used for testing\n",
    "        testingDF = dataDF_1.copy()\n",
    "        testingDF = testingDF.iloc[\n",
    "            int(TRAINING_SPLIT_PERCENT * len(dataDF_1)):]\n",
    "\n",
    "        print \"Num of Rows in Source:Training:Test\", len(dataDF_1), len(dataDF), len(testingDF)\n",
    "\n",
    "        print \"Stemming, stopwords removal from data\"\n",
    "        snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "        dataDF[\"Transformed text\"] = dataDF.apply(\n",
    "            lambda row: extract_words(row['text'], snowball, True), axis=1)\n",
    "\n",
    "        print \"Building vocabulary\"\n",
    "        vocabulary = build_vocabulary(dataDF[\"Transformed text\"])\n",
    "\n",
    "        print \"Calculating priors\"\n",
    "        label_priors = calculate_prior_probabilities(dataDF)\n",
    "        prior_of_real = float(label_priors[\"REAL\"])/float(label_priors[\"REAL\"] + label_priors[\"FAKE\"])\n",
    "        prior_of_fake = float(label_priors[\"FAKE\"])/float(label_priors[\"REAL\"] + label_priors[\"FAKE\"])\n",
    "\n",
    "        print \"Building Label Word count Matrix\"\n",
    "        matrix = label_word_count_matrix(vocabulary, dataDF)\n",
    "\n",
    "        print \"Writing Trained Model to file\"\n",
    "        f = open(TRAINING_FILE, \"w\")\n",
    "        f.write(json.dumps(zip(vocabulary, matrix)))\n",
    "        f.close()\n",
    "\n",
    "        print \"Calculating Accuracy Results\"\n",
    "        results = evaluateForAccuracy(testingDF, prior_of_fake, prior_of_real)\n",
    "\n",
    "        print \"Writing resulting labels to file\"\n",
    "        f = open(\"../data/results_nb.json\", \"w\")\n",
    "        f.write(json.dumps(results))\n",
    "        f.close()\n",
    "    except IOError as err:\n",
    "        print str(err)\n",
    "    except UnicodeDecodeError as err:\n",
    "        print str(err)\n",
    "    except Exception as err:\n",
    "        print str(err)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
